{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hisham-Zain/FinRobot/blob/master/FinRL_DeepSeek_backtesting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7Cycmf3Zbok"
      },
      "source": [
        "# FinRL-DeepSeek. Backtest\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1oWbj4HgqHBg"
      },
      "source": [
        "# Part 1. Install Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V0RbVjUMoheu",
        "outputId": "177012b1-a2d2-41b5-e951-c03b40f9c5e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/benstaf/FinRL.git\n",
            "  Cloning https://github.com/benstaf/FinRL.git to /tmp/pip-req-build-7emh4l8y\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/benstaf/FinRL.git /tmp/pip-req-build-7emh4l8y\n",
            "  Resolved https://github.com/benstaf/FinRL.git to commit cd016b667da1860939b43bb77aba7ff4e35f780f\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git (from finrl==0.3.6)\n",
            "  Cloning https://github.com/AI4Finance-Foundation/ElegantRL.git to /tmp/pip-install-h8q6ujx5/elegantrl_f4db083671e64806b5a9e13059c8daf8\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/AI4Finance-Foundation/ElegantRL.git /tmp/pip-install-h8q6ujx5/elegantrl_f4db083671e64806b5a9e13059c8daf8\n",
            "  Resolved https://github.com/AI4Finance-Foundation/ElegantRL.git to commit 8ea76afc3e7f1564ae9f0e69e70254116d575fe9\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting alpaca-trade-api<4,>=3 (from finrl==0.3.6)\n",
            "  Downloading alpaca_trade_api-3.2.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting ccxt<4,>=3 (from finrl==0.3.6)\n",
            "  Downloading ccxt-3.1.60-py2.py3-none-any.whl.metadata (108 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.7/108.7 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting exchange-calendars<5,>=4 (from finrl==0.3.6)\n",
            "  Downloading exchange_calendars-4.10.1-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting jqdatasdk<2,>=1 (from finrl==0.3.6)\n",
            "  Downloading jqdatasdk-1.9.7-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting pyfolio<0.10,>=0.9 (from finrl==0.3.6)\n",
            "  Downloading pyfolio-0.9.2.tar.gz (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.1/91.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyportfolioopt<2,>=1 (from finrl==0.3.6)\n",
            "  Downloading pyportfolioopt-1.5.6-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting ray<3,>=2 (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading ray-2.46.0-cp311-cp311-manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: scikit-learn<2,>=1 in /usr/local/lib/python3.11/dist-packages (from finrl==0.3.6) (1.6.1)\n",
            "Collecting stable-baselines3>=2.0.0a5 (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading stable_baselines3-2.6.1a1-py3-none-any.whl.metadata (4.8 kB)\n",
            "Collecting stockstats<0.6,>=0.5 (from finrl==0.3.6)\n",
            "  Downloading stockstats-0.5.4-py2.py3-none-any.whl.metadata (26 kB)\n",
            "Collecting wrds<4,>=3 (from finrl==0.3.6)\n",
            "  Downloading wrds-3.3.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting yfinance<0.3,>=0.2 (from finrl==0.3.6)\n",
            "  Downloading yfinance-0.2.62-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: pandas>=0.18.1 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.0.2)\n",
            "Requirement already satisfied: requests<3,>2 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (2.32.3)\n",
            "Collecting urllib3<2,>1.24 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: websocket-client<2,>=0.56.0 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (1.8.0)\n",
            "Collecting websockets<11,>=9.0 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
            "Collecting msgpack==1.0.3 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading msgpack-1.0.3.tar.gz (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: aiohttp<4,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from alpaca-trade-api<4,>=3->finrl==0.3.6) (3.11.15)\n",
            "Collecting PyYAML==6.0.1 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting deprecation==2.1.0 (from alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.6) (25.0)\n",
            "Requirement already satisfied: setuptools>=60.9.0 in /usr/local/lib/python3.11/dist-packages (from ccxt<4,>=3->finrl==0.3.6) (75.2.0)\n",
            "Requirement already satisfied: certifi>=2018.1.18 in /usr/local/lib/python3.11/dist-packages (from ccxt<4,>=3->finrl==0.3.6) (2025.4.26)\n",
            "Requirement already satisfied: cryptography>=2.6.1 in /usr/lib/python3/dist-packages (from ccxt<4,>=3->finrl==0.3.6) (3.4.8)\n",
            "Collecting aiodns>=1.1.1 (from ccxt<4,>=3->finrl==0.3.6)\n",
            "  Downloading aiodns-3.4.0-py3-none-any.whl.metadata (4.7 kB)\n",
            "Requirement already satisfied: yarl>=1.7.2 in /usr/local/lib/python3.11/dist-packages (from ccxt<4,>=3->finrl==0.3.6) (1.20.0)\n",
            "Collecting pyluach (from exchange-calendars<5,>=4->finrl==0.3.6)\n",
            "  Downloading pyluach-2.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.11/dist-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (1.0.0)\n",
            "Requirement already satisfied: tzdata in /usr/local/lib/python3.11/dist-packages (from exchange-calendars<5,>=4->finrl==0.3.6) (2025.2)\n",
            "Collecting korean_lunar_calendar (from exchange-calendars<5,>=4->finrl==0.3.6)\n",
            "  Downloading korean_lunar_calendar-0.3.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from jqdatasdk<2,>=1->finrl==0.3.6) (1.17.0)\n",
            "Collecting SQLAlchemy>=1.2.8 (from jqdatasdk<2,>=1->finrl==0.3.6)\n",
            "  Downloading sqlalchemy-2.0.41-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n",
            "Collecting pymysql>=0.7.6 (from jqdatasdk<2,>=1->finrl==0.3.6)\n",
            "  Downloading PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting thriftpy2!=0.5.1,>=0.3.9 (from jqdatasdk<2,>=1->finrl==0.3.6)\n",
            "  Downloading thriftpy2-0.5.2.tar.gz (782 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m782.3/782.3 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ipython>=3.2.3 in /usr/local/lib/python3.11/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (7.34.0)\n",
            "Requirement already satisfied: matplotlib>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (3.10.0)\n",
            "Requirement already satisfied: pytz>=2014.10 in /usr/local/lib/python3.11/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (2025.2)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (1.15.3)\n",
            "Requirement already satisfied: seaborn>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from pyfolio<0.10,>=0.9->finrl==0.3.6) (0.13.2)\n",
            "Collecting empyrical>=0.5.0 (from pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading empyrical-0.5.5.tar.gz (52 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cvxpy>=1.1.19 (from pyportfolioopt<2,>=1->finrl==0.3.6)\n",
            "  Downloading cvxpy-1.6.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\n",
            "Collecting ecos<3.0.0,>=2.0.14 (from pyportfolioopt<2,>=1->finrl==0.3.6)\n",
            "  Downloading ecos-2.0.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: plotly<6.0.0,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from pyportfolioopt<2,>=1->finrl==0.3.6) (5.24.1)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (8.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (3.18.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (4.24.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (5.29.5)\n",
            "Collecting aiohttp_cors (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading aiohttp_cors-0.8.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting colorful (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading colorful-0.5.6-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Collecting py-spy>=0.2.0 (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (1.72.1)\n",
            "Collecting opencensus (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2.11.5)\n",
            "Requirement already satisfied: prometheus_client>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (0.22.1)\n",
            "Requirement already satisfied: smart_open in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (7.1.0)\n",
            "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading virtualenv-20.31.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting tensorboardX>=1.9 (from ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: pyarrow>=9.0.0 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (20.0.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<3,>=2->finrl==0.3.6) (2025.5.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2,>=1->finrl==0.3.6) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2,>=1->finrl==0.3.6) (3.6.0)\n",
            "Collecting gymnasium<1.2.0,>=0.29.1 (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading gymnasium-1.1.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (2.6.0+cpu)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.1.1)\n",
            "Collecting opencv-python (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting pygame (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading pygame-2.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting tensorboard>=2.9.1 (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (14.0.0)\n",
            "Collecting ale-py>=0.9.0 (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading ale_py-0.11.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (11.2.1)\n",
            "Collecting packaging (from deprecation==2.1.0->alpaca-trade-api<4,>=3->finrl==0.3.6)\n",
            "  Downloading packaging-24.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting psycopg2-binary<2.10,>=2.9 (from wrds<4,>=3->finrl==0.3.6)\n",
            "  Downloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting multitasking>=0.0.7 (from yfinance<0.3,>=0.2->finrl==0.3.6)\n",
            "  Downloading multitasking-0.0.11-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (4.3.8)\n",
            "Collecting frozendict>=2.3.4 (from yfinance<0.3,>=0.2->finrl==0.3.6)\n",
            "  Downloading frozendict-2.4.6-py311-none-any.whl.metadata (23 kB)\n",
            "Collecting peewee>=3.16.2 (from yfinance<0.3,>=0.2->finrl==0.3.6)\n",
            "  Downloading peewee-3.18.1.tar.gz (3.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.11/dist-packages (from yfinance<0.3,>=0.2->finrl==0.3.6) (4.13.4)\n",
            "Collecting curl_cffi>=0.7 (from yfinance<0.3,>=0.2->finrl==0.3.6)\n",
            "  Downloading curl_cffi-0.11.3-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
            "INFO: pip is looking at multiple versions of yfinance to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting yfinance<0.3,>=0.2 (from finrl==0.3.6)\n",
            "  Downloading yfinance-0.2.61-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "  Downloading yfinance-0.2.60-py2.py3-none-any.whl.metadata (5.8 kB)\n",
            "  Downloading yfinance-0.2.59-py2.py3-none-any.whl.metadata (5.7 kB)\n",
            "  Downloading yfinance-0.2.58-py2.py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting th (from elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.6)\n",
            "  Downloading th-0.4.1-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting pycares>=4.0.0 (from aiodns>=1.1.1->ccxt<4,>=3->finrl==0.3.6)\n",
            "  Downloading pycares-4.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4,>=3.8.3->alpaca-trade-api<4,>=3->finrl==0.3.6) (0.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.6) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4>=4.11.1->yfinance<0.3,>=0.2->finrl==0.3.6) (4.14.0)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from curl_cffi>=0.7->yfinance<0.3,>=0.2->finrl==0.3.6) (1.17.1)\n",
            "Collecting osqp>=0.6.2 (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6)\n",
            "  Downloading osqp-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting clarabel>=0.5.0 (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6)\n",
            "  Downloading clarabel-0.11.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
            "Collecting scs>=3.2.4.post1 (from cvxpy>=1.1.19->pyportfolioopt<2,>=1->finrl==0.3.6)\n",
            "  Downloading scs-3.2.7.post2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting pandas-datareader>=0.2 (from empyrical>=0.5.0->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading pandas_datareader-0.10.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<1.2.0,>=0.29.1->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Collecting jedi>=0.16 (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (5.2.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.19.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.9.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=1.4.0->pyfolio<0.10,>=0.9->finrl==0.3.6) (2.9.0.post0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly<6.0.0,>=5.0.0->pyportfolioopt<2,>=1->finrl==0.3.6) (9.1.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[default,tune]<3,>=2->finrl==0.3.6) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[default,tune]<3,>=2->finrl==0.3.6) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,<3->ray[default,tune]<3,>=2->finrl==0.3.6) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>2->alpaca-trade-api<4,>=3->finrl==0.3.6) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>2->alpaca-trade-api<4,>=3->finrl==0.3.6) (3.10)\n",
            "Collecting greenlet>=1 (from SQLAlchemy>=1.2.8->jqdatasdk<2,>=1->finrl==0.3.6)\n",
            "  Downloading greenlet-3.2.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/lib/python3/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.3.6)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
            "Collecting werkzeug>=1.0.1 (from tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6)\n",
            "  Downloading werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
            "Collecting Cython>=3.0.10 (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.6)\n",
            "  Using cached cython-3.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.9 kB)\n",
            "Collecting ply<4.0,>=3.4 (from thriftpy2!=0.5.1,>=0.3.9->jqdatasdk<2,>=1->finrl==0.3.6)\n",
            "  Downloading ply-3.11-py2.py3-none-any.whl.metadata (844 bytes)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.1.6)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3>=2.0.0a5->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (1.3.0)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema->ray<3,>=2->ray[default,tune]<3,>=2->finrl==0.3.6) (0.25.1)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.6)\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (2.25.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.0.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart_open->ray[default,tune]<3,>=2->finrl==0.3.6) (1.17.2)\n",
            "Collecting niltype<2.0,>=0.3 (from th->elegantrl@ git+https://github.com/AI4Finance-Foundation/ElegantRL.git->finrl==0.3.6)\n",
            "  Downloading niltype-1.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance<0.3,>=0.2->finrl==0.3.6) (2.22)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (1.70.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (1.26.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (2.38.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.8.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (0.1.2)\n",
            "Collecting lxml (from pandas-datareader>=0.2->empyrical>=0.5.0->pyfolio<0.10,>=0.9->finrl==0.3.6)\n",
            "  Downloading lxml-5.4.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=3.2.3->pyfolio<0.10,>=0.9->finrl==0.3.6) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]>=2.0.0a5->finrl==0.3.6) (3.0.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (4.9.1)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<3,>=2->finrl==0.3.6) (0.6.1)\n",
            "Downloading alpaca_trade_api-3.2.0-py3-none-any.whl (34 kB)\n",
            "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading PyYAML-6.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m757.7/757.7 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ccxt-3.1.60-py2.py3-none-any.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading exchange_calendars-4.10.1-py3-none-any.whl (200 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jqdatasdk-1.9.7-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyportfolioopt-1.5.6-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.46.0-cp311-cp311-manylinux2014_x86_64.whl (68.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.5/68.5 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stable_baselines3-2.6.1a1-py3-none-any.whl (185 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.3/185.3 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stockstats-0.5.4-py2.py3-none-any.whl (21 kB)\n",
            "Downloading wrds-3.3.0-py3-none-any.whl (13 kB)\n",
            "Downloading yfinance-0.2.58-py2.py3-none-any.whl (113 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.7/113.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiodns-3.4.0-py3-none-any.whl (7.1 kB)\n",
            "Downloading ale_py-0.11.1-cp311-cp311-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m102.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading curl_cffi-0.11.3-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m115.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cvxpy-1.6.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ecos-2.0.14-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (220 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.1/220.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading frozendict-2.4.6-py311-none-any.whl (16 kB)\n",
            "Downloading gymnasium-1.1.1-py3-none-any.whl (965 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m965.4/965.4 kB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multitasking-0.0.11-py3-none-any.whl (8.5 kB)\n",
            "Downloading packaging-24.2-py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading psycopg2_binary-2.9.10-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sqlalchemy-2.0.41-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m118.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading virtualenv-20.31.2-py3-none-any.whl (6.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m117.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading websockets-10.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (107 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.4/107.4 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_cors-0.8.1-py3-none-any.whl (25 kB)\n",
            "Downloading colorful-0.5.6-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading korean_lunar_calendar-0.3.1-py3-none-any.whl (9.0 kB)\n",
            "Downloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python-4.11.0.86-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (63.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 MB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pygame-2.6.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m118.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyluach-2.2.0-py3-none-any.whl (25 kB)\n",
            "Downloading th-0.4.1-py3-none-any.whl (12 kB)\n",
            "Downloading clarabel-0.11.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached cython-3.1.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "Downloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Downloading greenlet-3.2.3-cp311-cp311-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (585 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m585.5/585.5 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading niltype-1.0.2-py3-none-any.whl (5.3 kB)\n",
            "Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Downloading osqp-1.0.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.3/345.3 kB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas_datareader-0.10.0-py3-none-any.whl (109 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.5/109.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycares-4.8.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (626 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.2/626.2 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scs-3.2.7.post2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m129.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m114.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lxml-5.4.0-cp311-cp311-manylinux_2_28_x86_64.whl (4.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: finrl, msgpack, pyfolio, elegantrl, empyrical, peewee, thriftpy2\n",
            "  Building wheel for finrl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for finrl: filename=finrl-0.3.6-py3-none-any.whl size=4700032 sha256=f6664846174d7d68c32f43998fb855a83b84288f60ddbab97d97ab0695276793\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0_8ycf1u/wheels/ed/f2/af/2ac8fe4e1481f79e0f6edc1d25dda7bd59ea4caa23d2070c78\n",
            "  Building wheel for msgpack (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for msgpack: filename=msgpack-1.0.3-cp311-cp311-linux_x86_64.whl size=15771 sha256=272328843632904ced6fb26842d4b4583870c6362cee408a57cd029fc2b67386\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/35/da/ed9b26b510235e00e3a3c3bab7bad97b59214729662255ab3d\n",
            "  Building wheel for pyfolio (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyfolio: filename=pyfolio-0.9.2-py3-none-any.whl size=88751 sha256=e867dcf2ab569b1cfcdc03c3af947e539bc29236e7097af9bd08f9877ed490e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/f9/af/9e/7c343b822164a3147a3d395a1bcd05041c520a3bc6398fe88e\n",
            "  Building wheel for elegantrl (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for elegantrl: filename=elegantrl-0.3.10-py3-none-any.whl size=272104 sha256=b410cfd8f782a16d965dd17f4bf7918490b26df72f94498b4fab16484a5e5846\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0_8ycf1u/wheels/9a/77/4d/6284111037b2dd64af9ef18d4d600d9c185cc2f6f09704e896\n",
            "  Building wheel for empyrical (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for empyrical: filename=empyrical-0.5.5-py3-none-any.whl size=39839 sha256=0f5e0387b7b51ad63107cd66012e615d93e862985777444348d21c6030a71584\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/1d/58/a7ae5ef5c8de7c4b769f24c2584f4706564921f031b16b9cb6\n",
            "  Building wheel for peewee (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for peewee: filename=peewee-3.18.1-cp311-cp311-linux_x86_64.whl size=886611 sha256=f1513debb00d6875bd94770554e1180d0075668b70a258a67667b5c5cce67545\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/cb/79/a133a0d1d75f318a96614ed7fb97bdf2f35a7b6c4d4e426e3f\n",
            "  Building wheel for thriftpy2 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for thriftpy2: filename=thriftpy2-0.5.2-cp311-cp311-linux_x86_64.whl size=1847808 sha256=5bb05d6a5ed1e668fd1dc926ba38789abe99a08e10a5e202785353af0bcf5ce2\n",
            "  Stored in directory: /root/.cache/pip/wheels/17/32/fa/51ae0364792430fb80858f4705c9e0cba3d6900f591f0c4495\n",
            "Successfully built finrl msgpack pyfolio elegantrl empyrical peewee thriftpy2\n",
            "Installing collected packages: py-spy, ply, peewee, opencensus-context, multitasking, msgpack, korean_lunar_calendar, farama-notifications, distlib, colorful, werkzeug, websockets, virtualenv, urllib3, tensorboard-data-server, PyYAML, pymysql, pyluach, pygame, psycopg2-binary, packaging, opencv-python, niltype, lxml, jedi, gymnasium, greenlet, frozendict, Cython, ale-py, thriftpy2, th, tensorboardX, tensorboard, SQLAlchemy, scs, pycares, osqp, ecos, deprecation, curl_cffi, clarabel, yfinance, wrds, stockstats, stable-baselines3, pandas-datareader, jqdatasdk, exchange-calendars, elegantrl, cvxpy, alpaca-trade-api, aiohttp_cors, aiodns, ray, pyportfolioopt, opencensus, empyrical, ccxt, pyfolio, finrl\n",
            "  Attempting uninstall: msgpack\n",
            "    Found existing installation: msgpack 1.1.0\n",
            "    Uninstalling msgpack-1.1.0:\n",
            "      Successfully uninstalled msgpack-1.1.0\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.4.0\n",
            "    Uninstalling urllib3-2.4.0:\n",
            "      Successfully uninstalled urllib3-2.4.0\n",
            "  Attempting uninstall: PyYAML\n",
            "    Found existing installation: PyYAML 6.0.2\n",
            "    Uninstalling PyYAML-6.0.2:\n",
            "      Successfully uninstalled PyYAML-6.0.2\n",
            "  Attempting uninstall: packaging\n",
            "    Found existing installation: packaging 25.0\n",
            "    Uninstalling packaging-25.0:\n",
            "      Successfully uninstalled packaging-25.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.18.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Cython-3.1.2 PyYAML-6.0.1 SQLAlchemy-2.0.41 aiodns-3.4.0 aiohttp_cors-0.8.1 ale-py-0.11.1 alpaca-trade-api-3.2.0 ccxt-3.1.60 clarabel-0.11.0 colorful-0.5.6 curl_cffi-0.11.3 cvxpy-1.6.5 deprecation-2.1.0 distlib-0.3.9 ecos-2.0.14 elegantrl-0.3.10 empyrical-0.5.5 exchange-calendars-4.10.1 farama-notifications-0.0.4 finrl-0.3.6 frozendict-2.4.6 greenlet-3.2.3 gymnasium-1.1.1 jedi-0.19.2 jqdatasdk-1.9.7 korean_lunar_calendar-0.3.1 lxml-5.4.0 msgpack-1.0.3 multitasking-0.0.11 niltype-1.0.2 opencensus-0.11.4 opencensus-context-0.1.3 opencv-python-4.11.0.86 osqp-1.0.4 packaging-24.2 pandas-datareader-0.10.0 peewee-3.18.1 ply-3.11 psycopg2-binary-2.9.10 py-spy-0.4.0 pycares-4.8.0 pyfolio-0.9.2 pygame-2.6.1 pyluach-2.2.0 pymysql-1.1.1 pyportfolioopt-1.5.6 ray-2.46.0 scs-3.2.7.post2 stable-baselines3-2.6.1a1 stockstats-0.5.4 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorboardX-2.6.2.2 th-0.4.1 thriftpy2-0.5.2 urllib3-1.26.20 virtualenv-20.31.2 websockets-10.4 werkzeug-3.1.3 wrds-3.3.0 yfinance-0.2.58\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/benstaf/FinRL.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzHWUxEZIEQy",
        "outputId": "58a42fcf-a2bb-4492-b5d6-b43218b6e90b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting selenium\n",
            "  Downloading selenium-4.33.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting webdriver-manager\n",
            "  Downloading webdriver_manager-4.0.2-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Collecting alpaca-py\n",
            "  Downloading alpaca_py-0.40.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting urllib3~=2.4.0 (from urllib3[socks]~=2.4.0->selenium)\n",
            "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting trio~=0.30.0 (from selenium)\n",
            "  Downloading trio-0.30.0-py3-none-any.whl.metadata (8.5 kB)\n",
            "Collecting trio-websocket~=0.12.2 (from selenium)\n",
            "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
            "Requirement already satisfied: certifi>=2025.4.26 in /usr/local/lib/python3.11/dist-packages (from selenium) (2025.4.26)\n",
            "Collecting typing_extensions~=4.13.2 (from selenium)\n",
            "  Downloading typing_extensions-4.13.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: websocket-client~=1.8.0 in /usr/local/lib/python3.11/dist-packages (from selenium) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (2.32.3)\n",
            "Collecting python-dotenv (from webdriver-manager)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from webdriver-manager) (24.2)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from alpaca-py) (1.0.3)\n",
            "Requirement already satisfied: pandas>=1.5.3 in /usr/local/lib/python3.11/dist-packages (from alpaca-py) (2.2.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.0.3 in /usr/local/lib/python3.11/dist-packages (from alpaca-py) (2.11.5)\n",
            "Collecting sseclient-py<2.0.0,>=1.7.2 (from alpaca-py)\n",
            "  Downloading sseclient_py-1.8.0-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from alpaca-py) (10.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (20.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.3->alpaca-py) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.3->alpaca-py) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5.3->alpaca-py) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.0.3->alpaca-py) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->webdriver-manager) (3.10)\n",
            "Requirement already satisfied: attrs>=23.2.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (25.3.0)\n",
            "Collecting sortedcontainers (from trio~=0.30.0->selenium)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting outcome (from trio~=0.30.0->selenium)\n",
            "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from trio~=0.30.0->selenium) (1.3.1)\n",
            "Collecting wsproto>=0.14 (from trio-websocket~=0.12.2->selenium)\n",
            "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from urllib3[socks]~=2.4.0->selenium) (1.7.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=1.5.3->alpaca-py) (1.17.0)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from wsproto>=0.14->trio-websocket~=0.12.2->selenium) (0.16.0)\n",
            "Downloading selenium-4.33.0-py3-none-any.whl (9.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading webdriver_manager-4.0.2-py2.py3-none-any.whl (27 kB)\n",
            "Downloading alpaca_py-0.40.1-py3-none-any.whl (121 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.7/121.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sseclient_py-1.8.0-py2.py3-none-any.whl (8.8 kB)\n",
            "Downloading trio-0.30.0-py3-none-any.whl (499 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m499.2/499.2 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
            "Downloading typing_extensions-4.13.2-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.8/45.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.7/128.7 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Installing collected packages: sseclient-py, sortedcontainers, xxhash, wsproto, urllib3, typing_extensions, python-dotenv, outcome, fsspec, dill, trio, multiprocess, webdriver-manager, trio-websocket, selenium, datasets, alpaca-py\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.26.20\n",
            "    Uninstalling urllib3-1.26.20:\n",
            "      Successfully uninstalled urllib3-1.26.20\n",
            "  Attempting uninstall: typing_extensions\n",
            "    Found existing installation: typing_extensions 4.14.0\n",
            "    Uninstalling typing_extensions-4.14.0:\n",
            "      Successfully uninstalled typing_extensions-4.14.0\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.5.1\n",
            "    Uninstalling fsspec-2025.5.1:\n",
            "      Successfully uninstalled fsspec-2025.5.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "alpaca-trade-api 3.2.0 requires urllib3<2,>1.24, but you have urllib3 2.4.0 which is incompatible.\n",
            "google-genai 1.18.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed alpaca-py-0.40.1 datasets-3.6.0 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 outcome-1.3.0.post0 python-dotenv-1.1.0 selenium-4.33.0 sortedcontainers-2.4.0 sseclient-py-1.8.0 trio-0.30.0 trio-websocket-0.12.2 typing_extensions-4.13.2 urllib3-2.4.0 webdriver-manager-4.0.2 wsproto-1.2.0 xxhash-3.5.0\n"
          ]
        }
      ],
      "source": [
        "!pip install selenium webdriver-manager alpaca-py datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Download environments: https://github.com/benstaf/FinRL_DeepSeek\n",
        "#And trading agents: https://huggingface.co/benstaf/Trading_agents"
      ],
      "metadata": {
        "id": "7r6aAYR1jOdN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOdUAKRe5lv4",
        "outputId": "4c089529-3e22-47ac-9b47-af66a3451679"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'FinRL_LLM'\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "cd FinRL_LLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mqfBOKz-qJYF",
        "outputId": "7631faec-940c-4f0e-dd76-c6457eb5ef06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'env_stocktrading'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-fdc2b005d6d2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#from finrl.agents.stablebaselines3.models import DRLAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mfinrl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mINDICATORS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAINED_MODEL_DIR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0menv_stocktrading\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStockTradingEnv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'env_stocktrading'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "#from stable_baselines3 import A2C, DDPG, PPO, SAC, TD3\n",
        "\n",
        "#from finrl.agents.stablebaselines3.models import DRLAgent\n",
        "from finrl.config import INDICATORS, TRAINED_MODEL_DIR\n",
        "from env_stocktrading import StockTradingEnv\n",
        "\n",
        "\n",
        "# Import PPO-DeepSeek environments\n",
        "from env_stocktrading_llm import StockTradingEnv as StockTradingEnv_llm\n",
        "from env_stocktrading_llm_1 import StockTradingEnv as StockTradingEnv_llm_1\n",
        "from env_stocktrading_llm_01 import StockTradingEnv as StockTradingEnv_llm_01\n",
        "\n",
        "# Import CPPO-DeepSeek risk environments\n",
        "from env_stocktrading_llm_risk import StockTradingEnv as StockTradingEnv_llm_risk\n",
        "from env_stocktrading_llm_risk_1 import StockTradingEnv as StockTradingEnv_llm_risk_1\n",
        "from env_stocktrading_llm_risk_01 import StockTradingEnv as StockTradingEnv_llm_risk_01\n",
        "\n",
        "#from env_stocktrading_llm import StockTradingEnv as StockTradingEnv_llm\n",
        "\n",
        "#from env_stocktrading_llm_risk import StockTradingEnv as StockTradingEnv_llm_risk\n",
        "\n",
        "\n",
        "#from finrl.meta.env_stock_trading.env_stocktrading_llm import StockTradingEnv\n",
        "from finrl.meta.preprocessor.yahoodownloader import YahooDownloader\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mUF2P4hmqVjh"
      },
      "source": [
        "# Part 2. Backtesting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "UMzVSoRtdhUu",
        "outputId": "6dc722fb-62ff-4e5d-efff-1cfb0ba4f565"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (<ipython-input-6-714befc7b44c>, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-714befc7b44c>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    from Huggging Face :\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# from Huggging Face :\n",
        "dataset = load_dataset(\"benstaf/nasdaq_2013_2023\", data_files='trade_data_deepseek_sentiment_2019_2023.csv')\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "trade = pd.DataFrame(dataset['train'])\n",
        "\n",
        "#trade= pd.read_csv('/content/machine_learning/trade_data_qwen_sentiment.csv')\n",
        "\n",
        "trade = trade.drop('Unnamed: 0',axis=1)\n",
        "\n",
        "# Create a new index based on unique dates\n",
        "unique_dates = trade['date'].unique()\n",
        "date_to_idx = {date: idx for idx, date in enumerate(unique_dates)}\n",
        "\n",
        "# Create new index based on the date mapping\n",
        "trade['new_idx'] = trade['date'].map(date_to_idx)\n",
        "\n",
        "# Set this as the index\n",
        "trade = trade.set_index('new_idx')\n",
        "\n",
        "\n",
        "#missing values with 0\n",
        "trade['llm_sentiment'].fillna(0, inplace=True)\n",
        "trade_llm=trade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kql-lIiXkFWX"
      },
      "outputs": [],
      "source": [
        "\n",
        "#trade = pd.read_csv('/content/machine_learning/trade_data_qwen_risk.csv')\n",
        "\n",
        "# from Huggging Face :\n",
        "dataset = load_dataset(\"benstaf/nasdaq_2013_2023\", data_files='trade_data_deepseek_risk_2019_2023.csv')\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "trade = pd.DataFrame(dataset['train'])\n",
        "\n",
        "trade = trade.drop('Unnamed: 0',axis=1)\n",
        "\n",
        "# Create a new index based on unique dates\n",
        "unique_dates = trade['date'].unique()\n",
        "date_to_idx = {date: idx for idx, date in enumerate(unique_dates)}\n",
        "\n",
        "# Create new index based on the date mapping\n",
        "trade['new_idx'] = trade['date'].map(date_to_idx)\n",
        "\n",
        "# Set this as the index\n",
        "trade = trade.set_index('new_idx')\n",
        "\n",
        "\n",
        "#missing values with 0\n",
        "trade['llm_sentiment'].fillna(0, inplace=True)\n",
        "#missing values with 3\n",
        "trade['llm_risk'].fillna(3, inplace=True)\n",
        "trade_llm_risk=trade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8cwiKJUcLqB"
      },
      "outputs": [],
      "source": [
        "#trade = pd.read_csv('/content/machine_learning/trade_data_qwen_risk.csv')\n",
        "\n",
        "# from Huggging Face :\n",
        "dataset = load_dataset(\"benstaf/nasdaq_2013_2023\", data_files='trade_data_2019_2023.csv')\n",
        "\n",
        "# Convert to pandas DataFrame\n",
        "trade = pd.DataFrame(dataset['train'])\n",
        "\n",
        "trade = trade.drop('Unnamed: 0',axis=1)\n",
        "\n",
        "# Create a new index based on unique dates\n",
        "unique_dates = trade['date'].unique()\n",
        "date_to_idx = {date: idx for idx, date in enumerate(unique_dates)}\n",
        "\n",
        "# Create new index based on the date mapping\n",
        "trade['new_idx'] = trade['date'].map(date_to_idx)\n",
        "\n",
        "# Set this as the index\n",
        "trade = trade.set_index('new_idx')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5mmgQF_h1jQ"
      },
      "source": [
        "### Trading (Out-of-sample Performance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H_w3SaBAkKU"
      },
      "outputs": [],
      "source": [
        "stock_dimension = len(trade.tic.unique())\n",
        "state_space = 1 + 2 * stock_dimension + len(INDICATORS) * stock_dimension #+ stock_dimension # +LLM sentiment\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wywEqlA6eWLl"
      },
      "outputs": [],
      "source": [
        "stock_dimension_llm = len(trade_llm.tic.unique())\n",
        "state_space_llm = 1 + 2 * stock_dimension_llm + (1+len(INDICATORS)) * stock_dimension_llm #+ stock_dimension # +LLM sentiment\n",
        "print(f\"Stock Dimension: {stock_dimension_llm}, State Space: {state_space_llm}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bIXRb35Vk9nt"
      },
      "outputs": [],
      "source": [
        "stock_dimension = len(trade.tic.unique())\n",
        "state_space_llm_risk = 1 + 2 * stock_dimension + (2+len(INDICATORS)) * stock_dimension #+ stock_dimension # +LLM sentiment + LLM risk\n",
        "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space_llm_risk}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKNmQMqGAknW"
      },
      "outputs": [],
      "source": [
        "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
        "num_stock_shares = [0] * stock_dimension\n",
        "\n",
        "env_kwargs = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 1000000,\n",
        "    \"num_stock_shares\": num_stock_shares,\n",
        "    \"buy_cost_pct\": buy_cost_list,\n",
        "    \"sell_cost_pct\": sell_cost_list,\n",
        "    \"state_space\": state_space,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRa_6myXjcgB"
      },
      "outputs": [],
      "source": [
        "buy_cost_list_llm = sell_cost_list_llm = [0.001] * stock_dimension_llm\n",
        "num_stock_shares_llm = [0] * stock_dimension_llm\n",
        "\n",
        "env_kwargs_llm = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 1000000,\n",
        "    \"num_stock_shares\": num_stock_shares_llm,\n",
        "    \"buy_cost_pct\": buy_cost_list_llm,\n",
        "    \"sell_cost_pct\": sell_cost_list_llm,\n",
        "    \"state_space\": state_space_llm,\n",
        "    \"stock_dim\": stock_dimension_llm,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension_llm,\n",
        "    \"reward_scaling\": 1e-4\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zm8q9Nyvpz7t"
      },
      "outputs": [],
      "source": [
        "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
        "num_stock_shares = [0] * stock_dimension\n",
        "\n",
        "env_kwargs_llm_risk = {\n",
        "    \"hmax\": 100,\n",
        "    \"initial_amount\": 1000000,\n",
        "    \"num_stock_shares\": num_stock_shares,\n",
        "    \"buy_cost_pct\": buy_cost_list,\n",
        "    \"sell_cost_pct\": sell_cost_list,\n",
        "    \"state_space\": state_space_llm_risk,\n",
        "    \"stock_dim\": stock_dimension,\n",
        "    \"tech_indicator_list\": INDICATORS,\n",
        "    \"action_space\": stock_dimension,\n",
        "    \"reward_scaling\": 1e-4\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cIqoV0GSI52v"
      },
      "outputs": [],
      "source": [
        "e_trade_gym = StockTradingEnv(df = trade, turbulence_threshold = 70,risk_indicator_col='vix', **env_kwargs)\n",
        "# env_trade, obs_trade = e_trade_gym.get_sb_env()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "teDV4u6ytznT"
      },
      "outputs": [],
      "source": [
        "e_trade_llm_gym = StockTradingEnv_llm(df = trade_llm, turbulence_threshold = 70,risk_indicator_col='vix', **env_kwargs_llm)\n",
        "# env_trade, obs_trade = e_trade_gym.get_sb_env()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TxOX7sagAKOr"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Environment for PPO-DeepSeek 10%\n",
        "e_trade_llm_gym = StockTradingEnv_llm(df=trade_llm, turbulence_threshold=70, risk_indicator_col='vix', **env_kwargs_llm)\n",
        "\n",
        "# Environment for PPO-DeepSeek 1%\n",
        "e_trade_llm_gym_1 = StockTradingEnv_llm_1(df=trade_llm, turbulence_threshold=70, risk_indicator_col='vix', **env_kwargs_llm)\n",
        "\n",
        "# Environment for PPO-DeepSeek 0.1%\n",
        "e_trade_llm_gym_01 = StockTradingEnv_llm_01(df=trade_llm, turbulence_threshold=70, risk_indicator_col='vix', **env_kwargs_llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "57xQY44MpkKV"
      },
      "outputs": [],
      "source": [
        "e_trade_llm_risk_gym = StockTradingEnv_llm_risk(df = trade_llm_risk, turbulence_threshold = 70,risk_indicator_col='vix', **env_kwargs_llm_risk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "279nlH-vAqqf"
      },
      "outputs": [],
      "source": [
        "# Environment for CPPO-DeepSeek 10% risk\n",
        "e_trade_llm_risk_gym = StockTradingEnv_llm_risk(df=trade_llm_risk, turbulence_threshold=70, risk_indicator_col='vix', **env_kwargs_llm_risk)\n",
        "\n",
        "# Environment for CPPO-DeepSeek 1% risk\n",
        "e_trade_llm_risk_gym_1 = StockTradingEnv_llm_risk_1(df=trade_llm_risk, turbulence_threshold=70, risk_indicator_col='vix', **env_kwargs_llm_risk)\n",
        "\n",
        "# Environment for CPPO-DeepSeek 0.1% risk\n",
        "e_trade_llm_risk_gym_01 = StockTradingEnv_llm_risk_01(df=trade_llm_risk, turbulence_threshold=70, risk_indicator_col='vix', **env_kwargs_llm_risk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJhhdFjCqoxO"
      },
      "outputs": [],
      "source": [
        "observation_space=e_trade_gym.observation_space\n",
        "action_space=e_trade_gym.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8g4u7Vtn0T8f"
      },
      "outputs": [],
      "source": [
        "observation_space_llm=e_trade_llm_gym.observation_space\n",
        "action_space_llm=e_trade_llm_gym.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLy1zid3qonw"
      },
      "outputs": [],
      "source": [
        "observation_space_llm_risk=e_trade_llm_risk_gym.observation_space\n",
        "action_space_llm_risk=e_trade_llm_risk_gym.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4N_gisMeBaH-"
      },
      "outputs": [],
      "source": [
        "# Observation and action spaces for PPO-DeepSeek 10%\n",
        "observation_space_llm = e_trade_llm_gym.observation_space\n",
        "action_space_llm = e_trade_llm_gym.action_space\n",
        "\n",
        "# Observation and action spaces for PPO-DeepSeek 1%\n",
        "observation_space_llm_1 = e_trade_llm_gym_1.observation_space\n",
        "action_space_llm_1 = e_trade_llm_gym_1.action_space\n",
        "\n",
        "# Observation and action spaces for PPO-DeepSeek 0.1%\n",
        "observation_space_llm_01 = e_trade_llm_gym_01.observation_space\n",
        "action_space_llm_01 = e_trade_llm_gym_01.action_space\n",
        "\n",
        "# Observation and action spaces for CPPO-DeepSeek 10% risk\n",
        "observation_space_llm_risk = e_trade_llm_risk_gym.observation_space\n",
        "action_space_llm_risk = e_trade_llm_risk_gym.action_space\n",
        "\n",
        "# Observation and action spaces for CPPO-DeepSeek 1% risk\n",
        "observation_space_llm_risk_1 = e_trade_llm_risk_gym_1.observation_space\n",
        "action_space_llm_risk_1 = e_trade_llm_risk_gym_1.action_space\n",
        "\n",
        "# Observation and action spaces for CPPO-DeepSeek 0.1% risk\n",
        "observation_space_llm_risk_01 = e_trade_llm_risk_gym_01.observation_space\n",
        "action_space_llm_risk_01 = e_trade_llm_risk_gym_01.action_space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1JAAcJtiazG"
      },
      "outputs": [],
      "source": [
        "print(\"State shape:\", observation_space_llm.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QLfLLERWqam"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import scipy.signal\n",
        "from gymnasium.spaces import Box, Discrete\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.distributions.normal import Normal\n",
        "from torch.distributions.categorical import Categorical\n",
        "\n",
        "\n",
        "def combined_shape(length, shape=None):\n",
        "    if shape is None:\n",
        "        return (length,)\n",
        "    return (length, shape) if np.isscalar(shape) else (length, *shape)\n",
        "\n",
        "\n",
        "def mlp(sizes, activation, output_activation=nn.Identity):\n",
        "    layers = []\n",
        "    for j in range(len(sizes)-1):\n",
        "        act = activation if j < len(sizes)-2 else output_activation\n",
        "        layers += [nn.Linear(sizes[j], sizes[j+1]), act()]\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def count_vars(module):\n",
        "    return sum([np.prod(p.shape) for p in module.parameters()])\n",
        "\n",
        "\n",
        "def discount_cumsum(x, discount):\n",
        "    \"\"\"\n",
        "    magic from rllab for computing discounted cumulative sums of vectors.\n",
        "\n",
        "    input:\n",
        "        vector x,\n",
        "        [x0,\n",
        "         x1,\n",
        "         x2]\n",
        "\n",
        "    output:\n",
        "        [x0 + discount * x1 + discount^2 * x2,\n",
        "         x1 + discount * x2,\n",
        "         x2]\n",
        "    \"\"\"\n",
        "    return scipy.signal.lfilter([1], [1, float(-discount)], x[::-1], axis=0)[::-1]\n",
        "\n",
        "\n",
        "class Actor(nn.Module):\n",
        "\n",
        "    def _distribution(self, obs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def _log_prob_from_distribution(self, pi, act):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def forward(self, obs, act=None):\n",
        "        # Produce action distributions for given observations, and\n",
        "        # optionally compute the log likelihood of given actions under\n",
        "        # those distributions.\n",
        "        pi = self._distribution(obs)\n",
        "        logp_a = None\n",
        "        if act is not None:\n",
        "            logp_a = self._log_prob_from_distribution(pi, act)\n",
        "        return pi, logp_a\n",
        "\n",
        "\n",
        "class MLPCategoricalActor(Actor):\n",
        "\n",
        "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
        "        super().__init__()\n",
        "        self.logits_net = mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation)\n",
        "\n",
        "    def _distribution(self, obs):\n",
        "        logits = self.logits_net(obs)\n",
        "        return Categorical(logits=logits)\n",
        "\n",
        "    def _log_prob_from_distribution(self, pi, act):\n",
        "        return pi.log_prob(act)\n",
        "\n",
        "\n",
        "class MLPGaussianActor(Actor):\n",
        "\n",
        "    def __init__(self, obs_dim, act_dim, hidden_sizes, activation):\n",
        "        super().__init__()\n",
        "        log_std = -0.5 * np.ones(act_dim, dtype=np.float32)\n",
        "        self.log_std = torch.nn.Parameter(torch.as_tensor(log_std))\n",
        "        self.mu_net = mlp([obs_dim] + list(hidden_sizes) + [act_dim], activation)\n",
        "\n",
        "    def _distribution(self, obs):\n",
        "        mu = self.mu_net(obs)\n",
        "        std = torch.exp(self.log_std)\n",
        "        return Normal(mu, std)\n",
        "\n",
        "    def _log_prob_from_distribution(self, pi, act):\n",
        "        return pi.log_prob(act).sum(axis=-1)    # Last axis sum needed for Torch Normal distribution\n",
        "\n",
        "\n",
        "class MLPCritic(nn.Module):\n",
        "\n",
        "    def __init__(self, obs_dim, hidden_sizes, activation):\n",
        "        super().__init__()\n",
        "        self.v_net = mlp([obs_dim] + list(hidden_sizes) + [1], activation)\n",
        "\n",
        "    def forward(self, obs):\n",
        "        return torch.squeeze(self.v_net(obs), -1) # Critical to ensure v has right shape.\n",
        "\n",
        "\n",
        "\n",
        "class MLPActorCritic(nn.Module):\n",
        "    def __init__(self, observation_space, action_space,\n",
        "                 hidden_sizes=(64, 64), activation=nn.Tanh):\n",
        "        super().__init__()\n",
        "\n",
        "        obs_dim = observation_space.shape[0]\n",
        "\n",
        "        # policy builder depends on action space\n",
        "        if isinstance(action_space, Box):\n",
        "            self.pi = MLPGaussianActor(obs_dim, action_space.shape[0], hidden_sizes, activation)\n",
        "        elif isinstance(action_space, Discrete):\n",
        "            self.pi = MLPCategoricalActor(obs_dim, action_space.n, hidden_sizes, activation)\n",
        "\n",
        "        # build value function\n",
        "        self.v = MLPCritic(obs_dim, hidden_sizes, activation)\n",
        "\n",
        "    def step(self, obs):\n",
        "        with torch.no_grad():\n",
        "            pi = self.pi._distribution(obs)\n",
        "            a = pi.sample()\n",
        "            logp_a = self.pi._log_prob_from_distribution(pi, a)\n",
        "            v = self.v(obs)\n",
        "        return a.numpy(), v.numpy(), logp_a.numpy()\n",
        "\n",
        "    def act(self, obs):\n",
        "        return self.step(obs)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qn8dWL48IERC"
      },
      "outputs": [],
      "source": [
        "!dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yugg8DG4rWBF"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "loaded_ppo = MLPActorCritic(observation_space,action_space, hidden_sizes=(512, 512))\n",
        "loaded_ppo.load_state_dict(torch.load('/content/FinRL_LLM/trained_models/agent_ppo_100_epochs_20k_steps.pth'))\n",
        "#loaded_ppo.load_state_dict(torch.load('//content/agent_ppo_100_epochs_20k_steps.pth'))\n",
        "\n",
        "loaded_ppo.eval()  # Set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbQWc49CtpVg"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "loaded_cppo = MLPActorCritic(observation_space,action_space, hidden_sizes=(512, 512))\n",
        "loaded_cppo.load_state_dict(torch.load('/content/FinRL_LLM/trained_models/agent_cppo_100_epochs_20k_steps.pth'))\n",
        "#loaded_ppo.load_state_dict(torch.load('/kaggle/input/agent_cppo_25_epochs_20k_steps/pytorch/default/1/agent_ppo_25_epochs_20k_steps.pth'))\n",
        "\n",
        "loaded_cppo.eval()  # Set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CfkgT-0B0vkc"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "loaded_ppo_llm = MLPActorCritic(observation_space_llm,action_space_llm, hidden_sizes=(512, 512))\n",
        "loaded_ppo_llm.load_state_dict(torch.load('/content/FinRL_LLM/trained_models/agent_ppo_deepseek_100_epochs_20k_steps.pth'))\n",
        "#loaded_ppo_llm.load_state_dict(torch.load('/kaggle/input/agent_cppo_25_epochs_20k_steps/pytorch/default/1/agent_ppo_25_epochs_20k_steps.pth'))\n",
        "\n",
        "loaded_ppo_llm.eval()  # Set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sj8yXDlM3ttH"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "loaded_ppo_llama = MLPActorCritic(observation_space_llm,action_space_llm, hidden_sizes=(512, 512))\n",
        "loaded_ppo_llama.load_state_dict(torch.load('/content/FinRL_LLM/trained_models/agent_ppo_llama_100_epochs_20k_steps.pth'))\n",
        "\n",
        "loaded_ppo_llm.eval()  # Set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBJNjVeSIERC"
      },
      "outputs": [],
      "source": [
        "# Load the model\n",
        "loaded_cppo_llm_risk = MLPActorCritic(observation_space_llm_risk,action_space_llm_risk, hidden_sizes=(512, 512))\n",
        "loaded_cppo_llm_risk.load_state_dict(torch.load('/content/FinRL_LLM/trained_models/agent_cppo_deepseek_100_epochs_20k_steps.pth'))\n",
        "\n",
        "loaded_cppo_llm_risk.eval()  # Set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PSxZHm46CUI6"
      },
      "outputs": [],
      "source": [
        "# Load the PPO-DeepSeek 10% model\n",
        "loaded_ppo_llm = MLPActorCritic(observation_space_llm, action_space_llm, hidden_sizes=(512, 512))\n",
        "loaded_ppo_llm.load_state_dict(torch.load('/content/FinRL_LLM/trained_models/agent_ppo_deepseek_100_epochs_20k_steps.pth'))\n",
        "loaded_ppo_llm.eval()  # Set the model to evaluation mode\n",
        "\n",
        "\n",
        "# Load the PPO-DeepSeek 1% model\n",
        "loaded_ppo_llm_1 = MLPActorCritic(observation_space_llm_1, action_space_llm_1, hidden_sizes=(512, 512))\n",
        "loaded_ppo_llm_1.load_state_dict(torch.load('/content/FinRL_LLM/trained_models/agent_ppo_deepseek_100_epochs_20k_steps_1.pth'))\n",
        "loaded_ppo_llm_1.eval()\n",
        "\n",
        "# Load the PPO-DeepSeek 0.1% model\n",
        "loaded_ppo_llm_01 = MLPActorCritic(observation_space_llm_01, action_space_llm_01, hidden_sizes=(512, 512))\n",
        "loaded_ppo_llm_01.load_state_dict(torch.load('/content/FinRL_LLM/trained_models/agent_ppo_deepseek_100_epochs_20k_steps_01.pth'))\n",
        "loaded_ppo_llm_01.eval()\n",
        "\n",
        "# Load the CPPO-DeepSeek 10% risk model\n",
        "loaded_cppo_llm_risk = MLPActorCritic(observation_space_llm_risk, action_space_llm_risk, hidden_sizes=(512, 512))\n",
        "loaded_cppo_llm_risk.load_state_dict(torch.load('/content/FinRL_LLM/trained_models/agent_cppo_deepseek_100_epochs_20k_steps.pth'))\n",
        "loaded_cppo_llm_risk.eval()\n",
        "\n",
        "# Load the CPPO-DeepSeek 1% risk model\n",
        "loaded_cppo_llm_risk_1 = MLPActorCritic(observation_space_llm_risk_1, action_space_llm_risk_1, hidden_sizes=(512, 512))\n",
        "loaded_cppo_llm_risk_1.load_state_dict(torch.load('/content/FinRL_LLM/trained_models/agent_cppo_deepseek_100_epochs_20k_steps_1.pth'))\n",
        "loaded_cppo_llm_risk_1.eval()\n",
        "\n",
        "# Load the CPPO-DeepSeek 0.1% risk model\n",
        "loaded_cppo_llm_risk_01 = MLPActorCritic(observation_space_llm_risk_01, action_space_llm_risk_01, hidden_sizes=(512, 512))\n",
        "loaded_cppo_llm_risk_01.load_state_dict(torch.load('/content/FinRL_LLM/trained_models/agent_cppo_deepseek_100_epochs_20k_steps_01.pth'))\n",
        "loaded_cppo_llm_risk_01.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ElguyMU4JqH"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Load the model\n",
        "loaded_cppo_llama_risk = MLPActorCritic(observation_space_llm_risk,action_space_llm_risk, hidden_sizes=(512, 512))\n",
        "loaded_cppo_llama_risk.load_state_dict(torch.load('/content/FinRL_LLM/trained_models/agent_deepseek_20_epochs_20k_steps.pth'))\n",
        "\n",
        "loaded_cppo_llama_risk.eval()  # Set the model to evaluation mode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlWpTX1MP6Bp"
      },
      "outputs": [],
      "source": [
        "\n",
        "def DRL_prediction(act, environment):\n",
        "    import torch\n",
        "    _torch = torch\n",
        "\n",
        "    state, _ = environment.reset()\n",
        "    account_memory = []  # To store portfolio values\n",
        "    actions_memory = []  # To store actions taken\n",
        "    portfolio_distribution = []  # To store portfolio distribution\n",
        "    episode_total_assets = [environment.initial_amount]\n",
        "\n",
        "    with _torch.no_grad():\n",
        "        for i in range(len(environment.df.index.unique())):\n",
        "            s_tensor = _torch.as_tensor((state,), dtype=torch.float32, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "            a_tensor, _, _ = act.step(s_tensor)  # Compute action\n",
        "            action = a_tensor[0]  # Extract action\n",
        "\n",
        "            # Step through the environment\n",
        "            state, reward, done, _, _ = environment.step(action)\n",
        "\n",
        "            # Get stock prices for the current day\n",
        "            price_array = environment.df.loc[environment.day, \"close\"].values\n",
        "\n",
        "            # Stock holdings and cash balance\n",
        "            stock_holdings = environment.num_stock_shares\n",
        "            cash_balance = environment.asset_memory[-1]\n",
        "\n",
        "            # Calculate total portfolio value\n",
        "            total_asset = cash_balance + (price_array * stock_holdings).sum()\n",
        "\n",
        "            # Calculate portfolio distribution\n",
        "            stock_values = price_array * stock_holdings\n",
        "            total_invested = stock_values.sum()\n",
        "            distribution = stock_values / total_asset  # Fraction of each stock in the total portfolio\n",
        "            cash_fraction = cash_balance / total_asset\n",
        "\n",
        "            # Store results\n",
        "            episode_total_assets.append(total_asset)\n",
        "            account_memory.append(total_asset)\n",
        "            actions_memory.append(action)\n",
        "            portfolio_distribution.append({\"cash\": cash_fraction, \"stocks\": distribution.tolist()})\n",
        "\n",
        "       #     print(\"Total Asset Value:\", total_asset)\n",
        "        #    print(\"Portfolio Distribution:\", {\"cash\": cash_fraction, \"stocks\": distribution.tolist()})\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "    print(\"Test Finished!\")\n",
        "    return episode_total_assets, account_memory, actions_memory, portfolio_distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4kMJijJGHB9"
      },
      "outputs": [],
      "source": [
        "df_assets_cppo[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ogrWz13aP-FF"
      },
      "outputs": [],
      "source": [
        "df_assets_ppo, df_account_value_ppo, df_actions_ppo, df_portfolio_distribution_ppo = DRL_prediction(act=loaded_ppo, environment=e_trade_gym)\n",
        "#episode_total_assets, account_memory, actions_memory, portfolio_distribution = DRL_prediction(act=loaded_ppo, environment=e_trade_gym)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "be6M88NRIERD"
      },
      "outputs": [],
      "source": [
        "df_assets_cppo, df_account_value_cppo, df_actions_cppo, df_portfolio_distribution_cppo = DRL_prediction(act=loaded_cppo, environment=e_trade_gym)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5nrxQNBC_9y"
      },
      "outputs": [],
      "source": [
        "# Prediction for PPO-DeepSeek 10%\n",
        "df_assets_ppo_llm, df_account_value_ppo_llm, df_actions_ppo_llm, df_portfolio_distribution_ppo_llm = DRL_prediction(\n",
        "    act=loaded_ppo_llm, environment=e_trade_llm_gym\n",
        ")\n",
        "\n",
        "# Prediction for PPO-DeepSeek 1%\n",
        "df_assets_ppo_llm_1, df_account_value_ppo_llm_1, df_actions_ppo_llm_1, df_portfolio_distribution_ppo_llm_1 = DRL_prediction(\n",
        "    act=loaded_ppo_llm_1, environment=e_trade_llm_gym_1\n",
        ")\n",
        "\n",
        "# Prediction for PPO-DeepSeek 0.1%\n",
        "df_assets_ppo_llm_01, df_account_value_ppo_llm_01, df_actions_ppo_llm_01, df_portfolio_distribution_ppo_llm_01 = DRL_prediction(\n",
        "    act=loaded_ppo_llm_01, environment=e_trade_llm_gym_01\n",
        ")\n",
        "\n",
        "# Prediction for CPPO-DeepSeek 10% risk\n",
        "df_assets_cppo_llm_risk, df_account_value_cppo_llm_risk, df_actions_cppo_llm_risk, df_portfolio_distribution_cppo_llm_risk = DRL_prediction(\n",
        "    act=loaded_cppo_llm_risk, environment=e_trade_llm_risk_gym\n",
        ")\n",
        "\n",
        "# Prediction for CPPO-DeepSeek 1% risk\n",
        "df_assets_cppo_llm_risk_1, df_account_value_cppo_llm_risk_1, df_actions_cppo_llm_risk_1, df_portfolio_distribution_cppo_llm_risk_1 = DRL_prediction(\n",
        "   act=loaded_cppo_llm_risk_1, environment=e_trade_llm_risk_gym_1\n",
        ")\n",
        "\n",
        "# Prediction for CPPO-DeepSeek 0.1% risk\n",
        "df_assets_cppo_llm_risk_01, df_account_value_cppo_llm_risk_01, df_actions_cppo_llm_risk_01, df_portfolio_distribution_cppo_llm_risk_01 = DRL_prediction(\n",
        "   act=loaded_cppo_llm_risk_01, environment=e_trade_llm_risk_gym_01\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oekpOfawIERD"
      },
      "outputs": [],
      "source": [
        "df_assets_ppo_llm, df_account_value_ppo_llm, df_actions_ppo_llm, df_portfolio_distribution_ppo_llm = DRL_prediction(act=loaded_ppo_llm, environment=e_trade_llm_gym)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4c8HxVG5BgS"
      },
      "outputs": [],
      "source": [
        "df_assets_ppo_llama, df_account_value_ppo_llama, df_actions_ppo_llama, df_portfolio_distribution_ppo_llama= DRL_prediction(act=loaded_ppo_llama, environment=e_trade_llm_gym)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CG81RsbYt-2m"
      },
      "outputs": [],
      "source": [
        "df_assets_cppo_llm_risk, df_account_value_cppo_llm_risk, df_actions_cppo_llm_risk, df_portfolio_distribution_cppo_llm_risk = DRL_prediction(act=loaded_cppo_llm_risk, environment=e_trade_llm_risk_gym)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1odVtewE5gdA"
      },
      "outputs": [],
      "source": [
        "df_assets_cppo_llama_risk, df_account_value_cppo_llama_risk, df_actions_cppo_llama_risk, df_portfolio_distribution_cppo_llama_risk = DRL_prediction(act=loaded_cppo_llama_risk, environment=e_trade_llm_risk_gym)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5sgGe7g1HsL"
      },
      "source": [
        "# Part 4: NASDAQ 100 index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVe_ufxTY2CW"
      },
      "source": [
        "**Add** NASDAQ 100 index as a baseline to compare with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sACPzsI-6k8q"
      },
      "outputs": [],
      "source": [
        "TRAIN_START_DATE = '2013-01-01'\n",
        "TRAIN_END_DATE = '2018-12-31'\n",
        "TRADE_START_DATE = '2019-01-01'\n",
        "TRADE_END_DATE = '2023-12-31'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TuszW-OB1K0m"
      },
      "outputs": [],
      "source": [
        "df_dji = YahooDownloader(\n",
        "    start_date=TRADE_START_DATE, end_date=TRADE_END_DATE, ticker_list=[\"ndx\"]\n",
        ").fetch_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PyYTWH4wFRRF"
      },
      "outputs": [],
      "source": [
        "len(df_dji)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kA19bifsGMqb"
      },
      "outputs": [],
      "source": [
        "df_dji[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q3RXz72U1VbV"
      },
      "outputs": [],
      "source": [
        "df_dji = df_dji[[\"date\", \"close\"]]\n",
        "fst_day = df_dji[\"close\"][0]\n",
        "dji = pd.merge(\n",
        "    df_dji[\"date\"],\n",
        "    df_dji[\"close\"].div(fst_day).mul(1000000),\n",
        "    how=\"outer\",\n",
        "    left_index=True,\n",
        "    right_index=True,\n",
        ").set_index(\"date\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oas3Sr27etXW"
      },
      "outputs": [],
      "source": [
        "fst_day = df_dji[\"close\"].iloc[0]  # Safely get the first value\n",
        "df_dji_normalized_close = list(df_dji[\"close\"].div(fst_day).mul(1000000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is9cHiyVIERF"
      },
      "outputs": [],
      "source": [
        "len(df_dji_normalized_close),"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6vvNSC6h1jZ"
      },
      "source": [
        "<a id='4'></a>\n",
        "# Part 5: Backtesting Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQuc5hI9Yklt"
      },
      "source": [
        "Now, everything is ready, we can plot the backtest result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fr1r8E3IcLzJ"
      },
      "outputs": [],
      "source": [
        "fst_day_ppo = df_assets_ppo[1]  # Safely get the first value\n",
        "df_assets_ppo_series = pd.Series(df_assets_ppo[1:])\n",
        "df_ppo_normalized_close = list(df_assets_ppo_series.div(fst_day_ppo).mul(1000000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcQMDRq9zRqU"
      },
      "outputs": [],
      "source": [
        "# Normalize PPO-DeepSeek 10%\n",
        "fst_day_ppo_llm = df_assets_ppo_llm[1]  # Safely get the first value\n",
        "df_assets_ppo_llm_series = pd.Series(df_assets_ppo_llm[1:])\n",
        "df_ppo_llm_normalized_close = list(df_assets_ppo_llm_series.div(fst_day_ppo_llm).mul(1000000))\n",
        "\n",
        "# Normalize PPO-DeepSeek 1%\n",
        "fst_day_ppo_llm_1 = df_assets_ppo_llm_1[1]  # Safely get the first value\n",
        "df_assets_ppo_llm_series_1 = pd.Series(df_assets_ppo_llm_1[1:])\n",
        "df_ppo_llm_normalized_close_1 = list(df_assets_ppo_llm_series_1.div(fst_day_ppo_llm_1).mul(1000000))\n",
        "\n",
        "# Normalize PPO-DeepSeek 0.1%\n",
        "#fst_day_ppo_llm_01 = df_assets_ppo_llm_01[1]  # Safely get the first value\n",
        "#df_assets_ppo_llm_series_01 = pd.Series(df_assets_ppo_llm_01[1:])\n",
        "#df_ppo_llm_normalized_close_01 = list(df_assets_ppo_llm_series_01.div(fst_day_ppo_llm_01).mul(1000000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AV0EBpPC81Ea"
      },
      "outputs": [],
      "source": [
        "# prompt: repeat the same renormalization as above for cppo, cppo_llm_risk and ppo_llm\n",
        "\n",
        "fst_day_ppo_llama = df_assets_ppo_llama[1]  # Safely get the first value\n",
        "df_assets_ppo_llama_series = pd.Series(df_assets_ppo_llama[1:])\n",
        "df_ppo_llama_normalized_close = list(df_assets_ppo_llama_series.div(fst_day_ppo_llama).mul(1000000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G7r6-O6rz23T"
      },
      "outputs": [],
      "source": [
        "\n",
        "fst_day_cppo = df_assets_cppo[1]  # Safely get the first value\n",
        "df_assets_cppo_series = pd.Series(df_assets_cppo[1:])\n",
        "df_cppo_normalized_close = list(df_assets_cppo_series.div(fst_day_cppo).mul(1000000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Xzfz5kzxRs2"
      },
      "outputs": [],
      "source": [
        "# Normalize CPPO-DeepSeek 10%\n",
        "fst_day_cppo_llm_risk = df_assets_cppo_llm_risk[1]  # Safely get the first value\n",
        "df_assets_cppo_llm_risk_series = pd.Series(df_assets_cppo_llm_risk[1:])\n",
        "df_cppo_llm_risk_normalized_close = list(df_assets_cppo_llm_risk_series.div(fst_day_cppo_llm_risk).mul(1000000))\n",
        "\n",
        "# Normalize CPPO-DeepSeek 1%\n",
        "fst_day_cppo_llm_risk_1 = df_assets_cppo_llm_risk_1[1]  # Safely get the first value\n",
        "df_assets_cppo_llm_risk_series_1 = pd.Series(df_assets_cppo_llm_risk_1[1:])\n",
        "df_cppo_llm_risk_normalized_close_1 = list(df_assets_cppo_llm_risk_series_1.div(fst_day_cppo_llm_risk_1).mul(1000000))\n",
        "\n",
        "# Normalize CPPO-DeepSeek 0.1%\n",
        "fst_day_cppo_llm_risk_01 = df_assets_cppo_llm_risk_01[1]  # Safely get the first value\n",
        "df_assets_cppo_llm_risk_series_01 = pd.Series(df_assets_cppo_llm_risk_01[1:])\n",
        "df_cppo_llm_risk_normalized_close_01 = list(df_assets_cppo_llm_risk_series_01.div(fst_day_cppo_llm_risk_01).mul(1000000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmp4VJVv84o1"
      },
      "outputs": [],
      "source": [
        "# prompt: repeat the same renormalization as above for cppo, cppo_llama_risk and ppo_llama\n",
        "\n",
        "fst_day_cppo_llama_risk = df_assets_cppo_llama_risk[1]  # Safely get the first value\n",
        "df_assets_cppo_llama_risk_series = pd.Series(df_assets_cppo_llama_risk[1:])\n",
        "df_cppo_llama_risk_normalized_close = list(df_assets_cppo_llama_risk_series.div(fst_day_cppo_llama_risk).mul(1000000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hz0VfttJ4rmr"
      },
      "outputs": [],
      "source": [
        "len(trade['date'].drop_duplicates().values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_34-nMA5IERH"
      },
      "outputs": [],
      "source": [
        "def filter_to_common_dates(trade, df_dji, df_assets_ppo, df_dji_normalized_close):\n",
        "    \"\"\"\n",
        "    Filters df_assets_ppo and df_dji_normalized_close based on the common dates from trade and df_dji.\n",
        "\n",
        "    Parameters:\n",
        "        trade (pd.DataFrame): DataFrame containing a 'date' column for the trade data.\n",
        "        df_dji (pd.DataFrame): DataFrame containing a 'date' column for DJI data.\n",
        "        df_assets_ppo (list or array-like): Values corresponding to trade['date'].\n",
        "        df_dji_normalized_close (list or array-like): Values corresponding to df_dji['date'].\n",
        "\n",
        "    Returns:\n",
        "        pd.Series, pd.Series: Filtered series for df_assets_ppo and df_dji_normalized_close.\n",
        "    \"\"\"\n",
        "    # Extract unique trading dates from trade and DJI dates\n",
        "    trade_dates = pd.to_datetime(trade['date'].unique())\n",
        "    dji_dates = pd.to_datetime(df_dji['date'].unique())\n",
        "\n",
        "\n",
        "  #  first_date = trade_dates[0]\n",
        "   # date_before_first = first_date - pd.DateOffset(days=1)\n",
        "\n",
        "# Prepend the date before the first date to trade_dates\n",
        "    #trade_dates = pd.DatetimeIndex([date_before_first] + trade_dates.tolist())\n",
        "\n",
        "    # Convert inputs to pandas Series with their respective dates as indices\n",
        "    df_assets_ppo_series = pd.Series(df_assets_ppo, index=trade_dates)\n",
        "    df_dji_normalized_close_series = pd.Series(df_dji_normalized_close, index=dji_dates)\n",
        "\n",
        "    # Find the common dates\n",
        "    common_dates = trade_dates.intersection(dji_dates)\n",
        "\n",
        "    # Filter both series to the common dates\n",
        "    df_assets_ppo_filtered = df_assets_ppo_series.reindex(common_dates)\n",
        "    df_dji_normalized_close_filtered = df_dji_normalized_close_series.reindex(common_dates)\n",
        "\n",
        "    # Return the filtered series\n",
        "    return df_assets_ppo_filtered, df_dji_normalized_close_filtered, common_dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Geb0XetR4Bii"
      },
      "outputs": [],
      "source": [
        "common_dates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZZbfGm94N9u"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ItlaDvMIERH"
      },
      "outputs": [],
      "source": [
        "df_assets_ppo_filtered, df_dji_normalized_close_filtered, common_dates = filter_to_common_dates(trade, df_dji, df_ppo_normalized_close, df_dji_normalized_close)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ta1O-fCIERH"
      },
      "outputs": [],
      "source": [
        "df_assets_cppo_filtered, df_dji_normalized_close_filtered, common_dates = filter_to_common_dates(trade, df_dji, df_cppo_normalized_close, df_dji_normalized_close)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4X15zzzNS5Su"
      },
      "outputs": [],
      "source": [
        "df_assets_ppo_llm_filtered, df_dji_normalized_close_filtered, common_dates = filter_to_common_dates(trade, df_dji, df_ppo_llm_normalized_close, df_dji_normalized_close)\n",
        "df_assets_ppo_llm_filtered_1, _, _ = filter_to_common_dates(\n",
        "    trade, df_dji, df_ppo_llm_normalized_close_1, df_dji_normalized_close\n",
        ")\n",
        "\n",
        "#df_assets_ppo_llm_filtered_01, _, _ = filter_to_common_dates(\n",
        "   # trade, df_dji, df_ppo_llm_normalized_close_01, df_dji_normalized_close\n",
        "#)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJAAl-dW-K-m"
      },
      "outputs": [],
      "source": [
        "df_assets_ppo_llama_filtered, df_dji_normalized_close_filtered, common_dates = filter_to_common_dates(trade, df_dji, df_ppo_llama_normalized_close, df_dji_normalized_close)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykZ0VWyQ_PsT"
      },
      "outputs": [],
      "source": [
        "df_assets_cppo_llm_risk_filtered, df_dji_normalized_close_filtered, common_dates = filter_to_common_dates(trade, df_dji, df_cppo_llm_risk_normalized_close, df_dji_normalized_close)\n",
        "df_assets_cppo_llm_risk_filtered_1, _, _ = filter_to_common_dates(\n",
        "    trade, df_dji, df_cppo_llm_risk_normalized_close_1, df_dji_normalized_close\n",
        ")\n",
        "\n",
        "df_assets_cppo_llm_risk_filtered_01, _, _ = filter_to_common_dates(\n",
        "    trade, df_dji, df_cppo_llm_risk_normalized_close_01, df_dji_normalized_close\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upQkVWmj-Y44"
      },
      "outputs": [],
      "source": [
        "#df_assets_cppo_llama_risk_filtered, df_dji_normalized_close_filtered, common_dates = filter_to_common_dates(trade, df_dji, df_cppo_llama_risk_normalized_close, df_dji_normalized_close)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Xq51tyCABS4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NqndNrzUIERI"
      },
      "outputs": [],
      "source": [
        "df_dji_normalized_close_filtered[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8r0skxjIERI"
      },
      "outputs": [],
      "source": [
        "result = pd.DataFrame(\n",
        "    {\n",
        "        \"PPO 100 epochs\": df_assets_ppo_filtered,\n",
        "        \"CPPO 100 epochs\": df_assets_cppo_filtered,\n",
        "        \"PPO-DeepSeek 100 epochs\": df_assets_ppo_llm_filtered,\n",
        "    #    \"PPO-Llama 100 epochs\": df_assets_ppo_llama_filtered,\n",
        "        \"CPPO-DeepSeek 100 epochs\": df_assets_cppo_llm_risk_filtered,\n",
        "    #    \"CPPO-Llama 20 epochs\": df_assets_cppo_llama_risk_filtered,\n",
        "        \"Nasdaq-100 index\": df_dji_normalized_close_filtered,\n",
        "    }\n",
        ")\n",
        "\n",
        "# Display the result\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tKdCaLL2wgT2"
      },
      "outputs": [],
      "source": [
        "result_ppo = pd.DataFrame(\n",
        "    {\n",
        "        \"PPO\": df_assets_ppo_filtered,\n",
        "        \"PPO-DeepSeek 10%\": df_assets_ppo_llm_filtered,\n",
        "        \"PPO-DeepSeek 1%\": df_assets_ppo_llm_filtered_1,\n",
        "        \"PPO-DeepSeek 0.1%\": df_assets_ppo_llm_filtered_01,\n",
        "        \"Nasdaq-100 index\": df_dji_normalized_close_filtered,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BA312yPAwjky"
      },
      "outputs": [],
      "source": [
        "result_cppo = pd.DataFrame(\n",
        "    {\n",
        "        #\"PPO 100 epochs\": df_assets_ppo_filtered,\n",
        "        \"CPPO\": df_assets_cppo_filtered,\n",
        "        \"CPPO-DeepSeek 10%\": df_assets_cppo_llm_risk_filtered,\n",
        "        \"CPPO-DeepSeek 1%\": df_assets_cppo_llm_risk_filtered_1,\n",
        "        \"CPPO-DeepSeek 0.1%\": df_assets_cppo_llm_risk_filtered_01,\n",
        "        \"Nasdaq-100 index\": df_dji_normalized_close_filtered,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DznFUPeo7qOe"
      },
      "outputs": [],
      "source": [
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Utility Functions\n",
        "def calculate_metric(returns_strategy, returns_benchmark, confidence_level=0.05, upside_confidence=0.95):\n",
        "    \"\"\"Calculate performance metrics: IR, CVaR, and Rachev Ratio.\"\"\"\n",
        "    excess_return = returns_strategy - returns_benchmark\n",
        "    ir = excess_return.mean() / excess_return.std()\n",
        "    var = np.percentile(returns_strategy, confidence_level * 100)\n",
        "    cvar = returns_strategy[returns_strategy <= var].mean()\n",
        "    upside_var = np.percentile(returns_strategy, upside_confidence * 100)\n",
        "    downside_var = var\n",
        "    rachev_ratio = returns_strategy[returns_strategy >= upside_var].mean() / abs(returns_strategy[returns_strategy <= downside_var].mean())\n",
        "    return {\"Information Ratio\": ir, \"CVaR\": cvar, \"Rachev Ratio\": rachev_ratio}\n",
        "\n",
        "def align_returns(result, col_strategy, col_benchmark):\n",
        "    \"\"\"Align returns for strategy and benchmark.\"\"\"\n",
        "    returns_strategy = result[col_strategy].pct_change().dropna()\n",
        "    returns_benchmark = result[col_benchmark].pct_change().dropna()\n",
        "    return returns_strategy.align(returns_benchmark, join=\"inner\")\n",
        "\n",
        "# Metrics Calculation\n",
        "def compute_metrics(result, strategies, benchmark, confidence_level=0.05, upside_confidence=0.95):\n",
        "    \"\"\"\n",
        "    Compute metrics for multiple strategies compared to a benchmark.\n",
        "\n",
        "    Parameters:\n",
        "        result (pd.DataFrame): DataFrame with strategies and benchmark columns.\n",
        "        strategies (list): List of strategy column names.\n",
        "        benchmark (str): Benchmark column name.\n",
        "        confidence_level (float): Confidence level for CVaR calculation.\n",
        "        upside_confidence (float): Confidence level for upside in Rachev Ratio.\n",
        "\n",
        "    Returns:\n",
        "        dict: Performance metrics for each strategy.\n",
        "    \"\"\"\n",
        "    metrics = {}\n",
        "    for strategy in strategies:\n",
        "        aligned_strategy, aligned_benchmark = align_returns(result, strategy, benchmark)\n",
        "        metrics[strategy] = calculate_metric(\n",
        "            aligned_strategy, aligned_benchmark, confidence_level, upside_confidence\n",
        "        )\n",
        "    return metrics\n",
        "\n",
        "# Plotting\n",
        "def plot_cumulative_returns(result, metrics, strategies, benchmark):\n",
        "    \"\"\"\n",
        "    Plot cumulative returns for strategies and benchmark with annotated metrics.\n",
        "\n",
        "    Parameters:\n",
        "        result (pd.DataFrame): DataFrame with strategies and benchmark.\n",
        "        metrics (dict): Performance metrics.\n",
        "        strategies (list): List of strategy column names.\n",
        "        benchmark (str): Benchmark column name.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for strategy in strategies:\n",
        "        cumulative_returns = (1 + result[strategy].pct_change().dropna()).cumprod()\n",
        "        plt.plot(cumulative_returns, label=f\"{strategy}\")\n",
        "    cumulative_benchmark = (1 + result[benchmark].pct_change().dropna()).cumprod()\n",
        "    plt.plot(cumulative_benchmark, label=f\"{benchmark} (Benchmark)\")\n",
        "    plt.title(\"Cumulative Returns with Performance Metrics\")\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Cumulative Return\")\n",
        "    plt.grid()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Edx_YvNyWB6W"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Utility Functions\n",
        "def calculate_metric(returns_strategy, returns_benchmark, confidence_level=0.05, upside_confidence=0.95):\n",
        "    \"\"\"Calculate performance metrics: IR, CVaR, and Rachev Ratio.\"\"\"\n",
        "    excess_return = returns_strategy - returns_benchmark\n",
        "    ir = excess_return.mean() / excess_return.std()\n",
        "    var = np.percentile(returns_strategy, confidence_level * 100)\n",
        "    cvar = returns_strategy[returns_strategy <= var].mean()\n",
        "    upside_var = np.percentile(returns_strategy, upside_confidence * 100)\n",
        "    downside_var = var\n",
        "    rachev_ratio = returns_strategy[returns_strategy >= upside_var].mean() / abs(returns_strategy[returns_strategy <= downside_var].mean())\n",
        "    return {\"Information Ratio\": ir, \"CVaR\": cvar, \"Rachev Ratio\": rachev_ratio}\n",
        "\n",
        "def align_returns(result, col_strategy, col_benchmark):\n",
        "    \"\"\"Align returns for strategy and benchmark.\"\"\"\n",
        "    returns_strategy = result[col_strategy].pct_change().dropna()\n",
        "    returns_benchmark = result[col_benchmark].pct_change().dropna()\n",
        "    return returns_strategy.align(returns_benchmark, join=\"inner\")\n",
        "\n",
        "# Metrics Calculation\n",
        "def compute_metrics(result, strategies, benchmark, confidence_level=0.05, upside_confidence=0.95):\n",
        "    \"\"\"\n",
        "    Compute metrics for multiple strategies compared to a benchmark.\n",
        "\n",
        "    Parameters:\n",
        "        result (pd.DataFrame): DataFrame with strategies and benchmark columns.\n",
        "        strategies (list): List of strategy column names.\n",
        "        benchmark (str): Benchmark column name.\n",
        "        confidence_level (float): Confidence level for CVaR calculation.\n",
        "        upside_confidence (float): Confidence level for upside in Rachev Ratio.\n",
        "\n",
        "    Returns:\n",
        "        dict: Performance metrics for each strategy.\n",
        "    \"\"\"\n",
        "    metrics = {}\n",
        "    for strategy in strategies:\n",
        "        aligned_strategy, aligned_benchmark = align_returns(result, strategy, benchmark)\n",
        "        metrics[strategy] = calculate_metric(\n",
        "            aligned_strategy, aligned_benchmark, confidence_level, upside_confidence\n",
        "        )\n",
        "    return metrics\n",
        "\n",
        "# Plotting\n",
        "def plot_cumulative_returns(result, metrics, strategies, benchmark):\n",
        "    \"\"\"\n",
        "    Plot cumulative returns for strategies and benchmark with annotated metrics.\n",
        "\n",
        "    Parameters:\n",
        "        result (pd.DataFrame): DataFrame with strategies and benchmark.\n",
        "        metrics (dict): Performance metrics.\n",
        "        strategies (list): List of strategy column names.\n",
        "        benchmark (str): Benchmark column name.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    for strategy in strategies:\n",
        "        cumulative_returns = (1 + result[strategy].pct_change().dropna()).cumprod()\n",
        "        plt.plot(cumulative_returns, label=f\"{strategy}\")\n",
        "    cumulative_benchmark = (1 + result[benchmark].pct_change().dropna()).cumprod()\n",
        "    plt.plot(cumulative_benchmark, label=f\"{benchmark} (Benchmark)\")\n",
        "    plt.title(\"Cumulative Returns with Performance Metrics\")\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Cumulative Return\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "# Example Usage\n",
        "# Assuming `result` DataFrame is prepared with \"PPO 25 epochs\", \"CPPO 25 epochs\", and \"Nasdaq-100 index\"\n",
        "strategies = [\n",
        "\"PPO 100 epochs\",\n",
        "\"CPPO 100 epochs\",\n",
        "\"PPO-DeepSeek 100 epochs\",\n",
        "#\"PPO-Llama 100 epochs\",\n",
        "\"CPPO-DeepSeek 100 epochs\"\n",
        "#\"CPPO-Llama 100 epochs\",\n",
        "]\n",
        "#strategies = [\"PPO 20 epochs\", \"CPPO 20 epochs\", \"CPPO-DeepSeek 20 epochs\"]\n",
        "benchmark = \"Nasdaq-100 index\"\n",
        "metrics = compute_metrics(result, strategies, benchmark)\n",
        "plot_cumulative_returns(result, metrics, strategies, benchmark)\n",
        "\n",
        "# Print metrics\n",
        "for strategy, strategy_metrics in metrics.items():\n",
        "    print(f\"{strategy} Metrics:\")\n",
        "    for metric_name, value in strategy_metrics.items():\n",
        "        print(f\"  {metric_name}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4r51JW_ezkc"
      },
      "outputs": [],
      "source": [
        "# Example Usage\n",
        "# Assuming `result` DataFrame is prepared with \"PPO 25 epochs\", \"CPPO 25 epochs\", and \"Nasdaq-100 index\"\n",
        "strategies = [\n",
        "\"PPO\",\n",
        "\"PPO-DeepSeek 10%\",\n",
        "\"PPO-DeepSeek 1%\",\n",
        "\"PPO-DeepSeek 0.1%\"\n",
        "#\"CPPO-Llama 100 epochs\",\n",
        "]\n",
        "#strategies = [\"PPO 20 epochs\", \"CPPO 20 epochs\", \"CPPO-DeepSeek 20 epochs\"]\n",
        "benchmark = \"Nasdaq-100 index\"\n",
        "metrics = compute_metrics(result_ppo, strategies, benchmark)\n",
        "plot_cumulative_returns(result_ppo, metrics, strategies, benchmark)\n",
        "\n",
        "# Print metrics\n",
        "for strategy, strategy_metrics in metrics.items():\n",
        "    print(f\"{strategy} Metrics:\")\n",
        "    for metric_name, value in strategy_metrics.items():\n",
        "        print(f\"  {metric_name}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aLc3y5jJfOkt"
      },
      "outputs": [],
      "source": [
        "# Example Usage\n",
        "# Assuming `result` DataFrame is prepared with \"PPO 25 epochs\", \"CPPO 25 epochs\", and \"Nasdaq-100 index\"\n",
        "strategies = [\n",
        "\"CPPO\",\n",
        "\"CPPO-DeepSeek 10%\",\n",
        "\"CPPO-DeepSeek 1%\",\n",
        "\"CPPO-DeepSeek 0.1%\"\n",
        "#\"CPPO-Llama 100 epochs\",\n",
        "]\n",
        "#strategies = [\"PPO 20 epochs\", \"CPPO 20 epochs\", \"CPPO-DeepSeek 20 epochs\"]\n",
        "benchmark = \"Nasdaq-100 index\"\n",
        "metrics = compute_metrics(result_cppo, strategies, benchmark)\n",
        "plot_cumulative_returns(result, metrics, strategies, benchmark)\n",
        "\n",
        "# Print metrics\n",
        "for strategy, strategy_metrics in metrics.items():\n",
        "    print(f\"{strategy} Metrics:\")\n",
        "    for metric_name, value in strategy_metrics.items():\n",
        "        print(f\"  {metric_name}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNaQ5xTuIERI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "def calculate_information_ratio(returns_strategy, returns_benchmark):\n",
        "    \"\"\"Calculate the Information Ratio (IR).\"\"\"\n",
        "    excess_return = returns_strategy - returns_benchmark\n",
        "    return excess_return.mean() / excess_return.std()\n",
        "\n",
        "def calculate_cvar(returns_strategy, confidence_level=0.05):\n",
        "    \"\"\"Calculate the Conditional Value at Risk (CVaR).\"\"\"\n",
        "    var = np.percentile(returns_strategy, confidence_level * 100)\n",
        "    return returns_strategy[returns_strategy <= var].mean()\n",
        "\n",
        "def calculate_rachev_ratio(returns_strategy, upside_confidence=0.95, downside_confidence=0.05):\n",
        "    \"\"\"Calculate the Rachev Ratio.\"\"\"\n",
        "    upside_var = np.percentile(returns_strategy, upside_confidence * 100)\n",
        "    downside_var = np.percentile(returns_strategy, downside_confidence * 100)\n",
        "    upside_mean = returns_strategy[returns_strategy >= upside_var].mean()\n",
        "    downside_mean = abs(returns_strategy[returns_strategy <= downside_var].mean())\n",
        "    return upside_mean / downside_mean\n",
        "\n",
        "def align_and_compute_metrics(result, confidence_level=0.05, upside_confidence=0.95):\n",
        "    \"\"\"\n",
        "    Align data for PPO, CPPO, and benchmark, and compute performance metrics.\n",
        "\n",
        "    Parameters:\n",
        "        result (pd.DataFrame): DataFrame with strategies and benchmark.\n",
        "        confidence_level (float): Confidence level for CVaR calculation.\n",
        "        upside_confidence (float): Confidence level for upside in Rachev Ratio.\n",
        "\n",
        "    Returns:\n",
        "        dict: Performance metrics for PPO and CPPO.\n",
        "    \"\"\"\n",
        "    # Calculate returns\n",
        "    returns_ppo = result[\"PPO 25 epochs\"].pct_change().dropna()\n",
        "    returns_cppo = result[\"CPPO 25 epochs\"].pct_change().dropna()\n",
        "    returns_benchmark = result[\"Nasdaq-100 index\"].pct_change().dropna()\n",
        "\n",
        "    # Align returns\n",
        "    returns_ppo, returns_benchmark_ppo = returns_ppo.align(returns_benchmark, join=\"inner\")\n",
        "    returns_cppo, returns_benchmark_cppo = returns_cppo.align(returns_benchmark, join=\"inner\")\n",
        "\n",
        "    # Compute metrics\n",
        "    metrics = {\n",
        "        \"PPO\": {\n",
        "            \"Information Ratio\": calculate_information_ratio(returns_ppo, returns_benchmark_ppo),\n",
        "            \"CVaR\": calculate_cvar(returns_ppo, confidence_level),\n",
        "            \"Rachev Ratio\": calculate_rachev_ratio(returns_ppo, upside_confidence, confidence_level),\n",
        "        },\n",
        "        \"CPPO\": {\n",
        "            \"Information Ratio\": calculate_information_ratio(returns_cppo, returns_benchmark_cppo),\n",
        "            \"CVaR\": calculate_cvar(returns_cppo, confidence_level),\n",
        "            \"Rachev Ratio\": calculate_rachev_ratio(returns_cppo, upside_confidence, confidence_level),\n",
        "        }\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "def plot_cumulative_returns(result, metrics):\n",
        "    \"\"\"\n",
        "    Plot cumulative returns for PPO, CPPO, and benchmark with annotated metrics.\n",
        "\n",
        "    Parameters:\n",
        "        result (pd.DataFrame): DataFrame with strategies and benchmark.\n",
        "        metrics (dict): Performance metrics.\n",
        "    \"\"\"\n",
        "    # Calculate cumulative returns\n",
        "    cumulative_ppo = (1 + result[\"PPO 25 epochs\"].pct_change().dropna()).cumprod()\n",
        "    cumulative_cppo = (1 + result[\"CPPO 25 epochs\"].pct_change().dropna()).cumprod()\n",
        "    cumulative_benchmark = (1 + result[\"Nasdaq-100 index\"].pct_change().dropna()).cumprod()\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(cumulative_ppo, label=f\"PPO 25 epochs (IR={metrics['PPO']['Information Ratio']:.4f})\")\n",
        "    plt.plot(cumulative_cppo, label=f\"CPPO 25 epochs (IR={metrics['CPPO']['Information Ratio']:.4f})\")\n",
        "    plt.plot(cumulative_benchmark, label=\"Nasdaq-100 index (Benchmark)\")\n",
        "    plt.title(\"Cumulative Returns with Performance Metrics\")\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"Date\")\n",
        "    plt.ylabel(\"Cumulative Return\")\n",
        "    plt.grid()\n",
        "    plt.show()\n",
        "\n",
        "# Example Usage\n",
        "# Assuming `result` DataFrame is prepared with \"PPO 25 epochs\", \"CPPO 25 epochs\", and \"Nasdaq-100 index\"\n",
        "metrics = align_and_compute_metrics(result)\n",
        "plot_cumulative_returns(result, metrics)\n",
        "\n",
        "# Print metrics\n",
        "for strategy, strategy_metrics in metrics.items():\n",
        "    print(f\"{strategy} Metrics:\")\n",
        "    for metric_name, value in strategy_metrics.items():\n",
        "        print(f\"  {metric_name}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcGXK17Yoy2m"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ... (your existing code to load data and calculate strategies) ...\n",
        "\n",
        "# Get unique trading dates from your trade data\n",
        "trade_dates = pd.to_datetime(trade['date'].unique())\n",
        "\n",
        "first_date = trade_dates[0]\n",
        "date_before_first = first_date - pd.DateOffset(days=1)\n",
        "\n",
        "# Prepend the date before the first date to trade_dates\n",
        "trade_dates = pd.DatetimeIndex([date_before_first] + trade_dates.tolist())\n",
        "\n",
        "\n",
        "# Reindex your strategy results to match the trading dates\n",
        "df_assets_ppo_series = pd.Series(df_assets_ppo, index=trade_dates)\n",
        "#df_dji_normalized_close_series = pd.Series(df_dji_normalized_close, index=trade_dates) # Convert to Series\n",
        "\n",
        "\n",
        "# 1. Get dates from df_dji (Yahoo Finance data)\n",
        "\n",
        "dji_dates = pd.to_datetime(df_dji['date'])\n",
        "\n",
        "# 2. Find the intersection of trade_dates and dji_dates\n",
        "common_dates = trade_dates.intersection(dji_dates)\n",
        "\n",
        "# 3. Reindex df_assets_ppo to keep only common_dates\n",
        "df_assets_ppo_series = pd.Series(df_assets_ppo, index=trade_dates).reindex(common_dates)\n",
        "\n",
        "# Reindex df_dji to match common_dates, forward-filling missing values (if any)\n",
        "df_dji_normalized_close_series = pd.Series(df_dji_normalized_close, index=common_dates)\n",
        "\n",
        "# Create the DataFrame with trading dates as the index\n",
        "result = pd.DataFrame(\n",
        "    {\n",
        "        \"PPO 70 epochs\": df_assets_ppo_series,\n",
        "        \"Nasdaq-100 index\": df_dji_normalized_close_series,\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FGP2SHLLo0B2"
      },
      "outputs": [],
      "source": [
        "result.to_csv('result_ppo_qwen_25_epochs_20k_steps.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUne9VHyxa3c"
      },
      "outputs": [],
      "source": [
        "# prompt: plot also the sharpe ratio and sortino ratio of nasdaq and ppo\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming 'result' DataFrame is already created as in your provided code\n",
        "\n",
        "# Calculate daily returns\n",
        "result['PPO 70 epochs_daily_return'] = result['PPO 70 epochs'].pct_change()\n",
        "result['Nasdaq-100 index_daily_return'] = result['Nasdaq-100 index'].pct_change()\n",
        "\n",
        "\n",
        "# Calculate Sharpe Ratio\n",
        "sharpe_ppo = np.sqrt(252) * (result['PPO 70 epochs_daily_return'].mean() / result['PPO 70 epochs_daily_return'].std())\n",
        "sharpe_nasdaq = np.sqrt(252) * (result['Nasdaq-100 index_daily_return'].mean() / result['Nasdaq-100 index_daily_return'].std())\n",
        "\n",
        "# Calculate Sortino Ratio (assuming a target return of 0)\n",
        "downside_returns_ppo = result['PPO 70 epochs_daily_return'].where(result['PPO 70 epochs_daily_return'] < 0, 0)\n",
        "downside_returns_nasdaq = result['Nasdaq-100 index_daily_return'].where(result['Nasdaq-100 index_daily_return'] < 0, 0)\n",
        "\n",
        "sortino_ppo = np.sqrt(252) * (result['PPO 70 epochs_daily_return'].mean() / downside_returns_ppo.std())\n",
        "sortino_nasdaq = np.sqrt(252) * (result['Nasdaq-100 index_daily_return'].mean() / downside_returns_nasdaq.std())\n",
        "\n",
        "\n",
        "#Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(result.index, result['PPO 70 epochs'], label='PPO 70 epochs')\n",
        "plt.plot(result.index, result['Nasdaq-100 index'], label='Nasdaq-100 index')\n",
        "plt.title('Portfolio Value Comparison')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Portfolio Value')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "\n",
        "# Move legend to upper left\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04VTLHqYVJxa"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming `df_assets_ppo_series` and `df_dji_normalized_close_series` are already created\n",
        "returns_strategy = df_assets_ppo_series.pct_change().dropna()\n",
        "returns_benchmark = df_dji_normalized_close_series.pct_change().dropna()\n",
        "\n",
        "# Align data\n",
        "returns_strategy, returns_benchmark = returns_strategy.align(returns_benchmark, join=\"inner\")\n",
        "\n",
        "# Information Ratio\n",
        "excess_return = returns_strategy - returns_benchmark\n",
        "information_ratio = excess_return.mean() / excess_return.std()\n",
        "\n",
        "# CVaR\n",
        "confidence_level = 0.05\n",
        "var = np.percentile(returns_strategy, confidence_level * 100)\n",
        "cvar = returns_strategy[returns_strategy <= var].mean()\n",
        "\n",
        "# Rachev Ratio\n",
        "upside_confidence = 0.95\n",
        "downside_confidence = 0.05\n",
        "upside_var = np.percentile(returns_strategy, upside_confidence * 100)\n",
        "downside_var = np.percentile(returns_strategy, downside_confidence * 100)\n",
        "rachev_ratio = returns_strategy[returns_strategy >= upside_var].mean() / abs(returns_strategy[returns_strategy <= downside_var].mean())\n",
        "\n",
        "# Print metrics\n",
        "print(f\"Information Ratio: {information_ratio:.4f}\")\n",
        "print(f\"CVaR (5%): {cvar:.4f}\")\n",
        "print(f\"Rachev Ratio: {rachev_ratio:.4f}\")\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Strategy and benchmark cumulative returns\n",
        "cumulative_strategy = (1 + returns_strategy).cumprod()\n",
        "cumulative_benchmark = (1 + returns_benchmark).cumprod()\n",
        "\n",
        "plt.plot(cumulative_strategy, label=\"PPO 25 epochs (Strategy)\")\n",
        "plt.plot(cumulative_benchmark, label=\"Nasdaq-100 index (Baseline)\")\n",
        "\n",
        "# Add metrics to the plot\n",
        "plt.title(f\"Performance Metrics:\\nIR={information_ratio:.4f}, CVaR (5%)={cvar:.4f}, Rachev={rachev_ratio:.4f}\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Cumulative Return\")\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SdVni8vROZPc"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# ... (your existing code to load data and calculate strategies) ...\n",
        "\n",
        "# Calculate daily returns (same as before)\n",
        "# ...\n",
        "\n",
        "# Calculate Sharpe and Sortino Ratios (same as before)\n",
        "# ...\n",
        "\n",
        "# Create a figure with two subplots\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))  # 1 row, 2 columns\n",
        "\n",
        "# --- Subplot 1: Sharpe Ratio Comparison ---\n",
        "ax1.bar(['PPO 70 epochs', 'Nasdaq-100 index'], [sharpe_ppo, sharpe_nasdaq], color=['blue', 'orange'])\n",
        "ax1.set_title('Sharpe Ratio Comparison')\n",
        "ax1.set_ylabel('Sharpe Ratio')\n",
        "\n",
        "# Add Sharpe ratio values as text on top of the bars\n",
        "for i, v in enumerate([sharpe_ppo, sharpe_nasdaq]):\n",
        "    ax1.text(i, v + 0.05, f\"{v:.2f}\", ha='center', va='bottom')\n",
        "\n",
        "# --- Subplot 2: Sortino Ratio Comparison ---\n",
        "ax2.bar(['PPO 70 epochs', 'Nasdaq-100 index'], [sortino_ppo, sortino_nasdaq], color=['green', 'red'])\n",
        "ax2.set_title('Sortino Ratio Comparison')\n",
        "ax2.set_ylabel('Sortino Ratio')\n",
        "\n",
        "# Add Sortino ratio values as text on top of the bars\n",
        "for i, v in enumerate([sortino_ppo, sortino_nasdaq]):\n",
        "    ax2.text(i, v + 0.05, f\"{v:.2f}\", ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTiRzW5wPfMX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5s-XdhNPfHi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rjihjOK7PfEJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RwutU7uAmAja"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(16, 6))\n",
        "for column in result.columns:\n",
        "    plt.plot(result.index, result[column], label=column)\n",
        "\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Portfolio Value\")\n",
        "plt.title(\"Backtesting Results (Tradable Days Only)\")\n",
        "\n",
        "\n",
        "# Get the first and last dates from the index\n",
        "first_day = result.index[0]\n",
        "last_day = result.index[-1]\n",
        "\n",
        "# Create a list of dates for ticks, including first, last, and every 15 days\n",
        "tick_dates = [first_day]  # Start with the first day\n",
        "current_date = first_day + pd.DateOffset(days=15)  # Add 15 days\n",
        "while current_date < last_day:\n",
        "    tick_dates.append(current_date)\n",
        "    current_date += pd.DateOffset(days=15)\n",
        "tick_dates.append(last_day)  # Add the last day\n",
        "\n",
        "\n",
        "# Remove December 13th if it's in the tick_dates list\n",
        "tick_dates = [d for d in tick_dates if d.strftime('%Y-%m-%d') != '2023-12-13']\n",
        "\n",
        "\n",
        "# Set x-axis ticks to the calculated tick_dates\n",
        "plt.xticks(tick_dates, [d.strftime('%Y-%m-%d') for d in tick_dates], rotation=45)\n",
        "\n",
        "\n",
        "#plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5v8A7qegWpR"
      },
      "outputs": [],
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
        "plt.figure()\n",
        "result.plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xRfrqK4RVfq"
      },
      "outputs": [],
      "source": [
        "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
        "plt.figure()\n",
        "result.plot()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "isSourceIdPinned": true,
          "modelId": 222180,
          "modelInstanceId": 200368,
          "sourceId": 234579,
          "sourceType": "modelInstanceVersion"
        },
        {
          "isSourceIdPinned": true,
          "modelId": 222227,
          "modelInstanceId": 200415,
          "sourceId": 234637,
          "sourceType": "modelInstanceVersion"
        }
      ],
      "dockerImageVersionId": 30839,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}